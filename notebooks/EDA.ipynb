{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EDA for Ecommerce Text Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/ecommerce-text-classifaction/data/ecommerceDataset.csv\",names = ['label', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of records with missing values\n",
    "\n",
    "print(f\"Number of records with missing values : {len(df) - len(df.dropna())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of duplicated records\n",
    "\n",
    "print(f\"Number of duplicate records : {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the number of records with missing values are 1, it is safe to drop that record\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are duplicated records which provide no meaning, we can drop those records also\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Label Encoding the target cols \n",
    "\n",
    "label_dict = {'Electronics': 0, 'Household': 1, 'Books': 2, 'Clothing & Accessories': 3}\n",
    "\n",
    "df.replace({'label': label_dict}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset by label\n",
    "data_e = df[df['label'] == 0] # Electronics\n",
    "data_h = df[df['label'] == 1] # Household\n",
    "data_b = df[df['label'] == 2] # Books\n",
    "data_c = df[df['label'] == 3] # Clothing & Accessories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Class Frequencies\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "values = np.array([len(data_e), len(data_h), len(data_b), len(data_c)])\n",
    "\n",
    "labels = ['Electronics', 'Household', 'Books', 'Clothing & Accessories']\n",
    "\n",
    "plt.title(\"Comparision of class frequencies\")\n",
    "\n",
    "fig = plt.pie(x=values,labels=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of characters in description\n",
    "\n",
    "data_e_char = data_e['description'].str.len()\n",
    "data_h_char = data_h['description'].str.len()\n",
    "data_b_char = data_b['description'].str.len()\n",
    "data_c_char = data_c['description'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize = (10, 8.4), sharey = False)\n",
    "\n",
    "sns.histplot(x = data_e_char, bins = 20, ax = ax[0, 0]).set_title('Class: Electronics')\n",
    "sns.histplot(x = data_h_char, bins = 20, ax = ax[0, 1]).set_title('Class: Household')\n",
    "sns.histplot(x = data_b_char, bins = 20, ax = ax[1, 0]).set_title('Class: Books')\n",
    "sns.histplot(x = data_c_char, bins = 20, ax = ax[1, 1]).set_title('Class: Clothing & Accessories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of words in description\n",
    "data_e_word = data_e['description'].str.split().map(lambda x: len(x))\n",
    "data_h_word = data_h['description'].str.split().map(lambda x: len(x))\n",
    "data_b_word = data_b['description'].str.split().map(lambda x: len(x))\n",
    "data_c_word = data_c['description'].str.split().map(lambda x: len(x))\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize = (10, 8.4), sharey = False)\n",
    "sns.histplot(x = data_e_word, bins = 20, ax = ax[0, 0]).set_title('Class: Electronics')\n",
    "sns.histplot(x = data_h_word, bins = 20, ax = ax[0, 1]).set_title('Class: Household')\n",
    "sns.histplot(x = data_b_word, bins = 20, ax = ax[1, 0]).set_title('Class: Books')\n",
    "sns.histplot(x = data_c_word, bins = 20, ax = ax[1, 1]).set_title('Class: Clothing & Accessories')\n",
    "\n",
    "fig.suptitle(\"Distribution of number of words in description\")\n",
    "for i in range(4):\n",
    "    ax[i // 2, i % 2].set_xlabel(\" \") if i // 2 == 0 else ax[i // 2, i % 2].set_xlabel(\"Number of words\")\n",
    "    if i % 2 != 0: ax[i // 2, i % 2].set_ylabel(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of average word-length in description\n",
    "data_e_avg = data_e['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "data_h_avg = data_h['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "data_b_avg = data_b['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "data_c_avg = data_c['description'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize = (10, 8.4), sharey = False)\n",
    "sns.histplot(x = data_e_avg, bins = 20, ax = ax[0, 0]).set_title('Class: Electronics')\n",
    "sns.histplot(x = data_h_avg, bins = 20, ax = ax[0, 1]).set_title('Class: Household')\n",
    "sns.histplot(x = data_b_avg, bins = 20, ax = ax[1, 0]).set_title('Class: Books')\n",
    "sns.histplot(x = data_c_avg, bins = 20, ax = ax[1, 1]).set_title('Class: Clothing & Accessories')\n",
    "\n",
    "fig.suptitle(\"Distribution of average word-length in description\")\n",
    "for i in range(4):\n",
    "    ax[i // 2, i % 2].set_xlabel(\" \") if i // 2 == 0 else ax[i // 2, i % 2].set_xlabel(\"Average word-length\")\n",
    "    if i % 2 != 0: ax[i // 2, i % 2].set_ylabel(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y = df.drop('label',axis=1),df['label']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "data_train = pd.concat([X_train,y_train],axis=1)\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_test,y_test,test_size=0.5,random_state=42)\n",
    "\n",
    "data_val = pd.concat([X_val,y_val],axis=1)\n",
    "\n",
    "data_test = pd.concat([X_test,y_test],axis=1)\n",
    "\n",
    "# Comparison of sizes of training set, validation set and test set\n",
    "values = np.array([len(data_train), len(data_val), len(data_test)])\n",
    "\n",
    "labels = ['Training set', 'Validation Set', 'Test set']\n",
    "\n",
    "plt.title(\"Comparision of sizes of training,validation and test set\")\n",
    "\n",
    "plt.pie(x=values,labels=labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting text to lowercase\n",
    "\n",
    "convert_to_lowercase = lambda text : text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing whitespaces \n",
    "\n",
    "remove_whitespace = lambda text : text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "remove_punctuation = lambda text : text.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"'\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removal of unicode characters\n",
    "\n",
    "import re\n",
    "\n",
    "remove_html = lambda text : re.compile(r'<.*?>').sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing emojis\n",
    "    \n",
    "remove_emoji = lambda text : re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE).sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing other unicode characters like http\n",
    "\n",
    "remove_http = lambda text : re.sub(r\"({})\".format(\"https?://\\S+|www\\.\\S+\"), \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Replacing acronyms\n",
    "\n",
    "acronyms_dict = pd.read_json(\"english_acronyms.json\", typ = 'series')\n",
    "\n",
    "acronyms_df = pd.DataFrame(acronyms_dict.items(), columns = ['acronym', 'original']).head()\n",
    "\n",
    "acronyms_list = list(acronyms_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert contractions in text\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    \n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "            \n",
    "    text_converted = \" \".join(words)\n",
    "    \n",
    "    return text_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Substitution of Contractions\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "contractions_dict = pd.read_json(\"english_contractions.json\", typ = 'series')\n",
    "\n",
    "df_contractions = pd.DataFrame(contractions_dict.items(), columns = ['contraction', 'original']).head()\n",
    "\n",
    "contractions_list = list(contractions_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_contractions(text):\n",
    "    words = []\n",
    "\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "\n",
    "    return text_converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"]\n",
    "\n",
    "all_stopwords = stops + addstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing stop words\n",
    "remove_stopwords = lambda text : \" \".join([word for word in regexp.tokenize(text) if word not in all_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correcting spelling\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def pyspellchecker(text):\n",
    "    word_list = regexp.tokenize(text)\n",
    "    \n",
    "    word_list_corrected = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in spell.unknown(word_list):\n",
    "            word_corrected = spell.correction(word)\n",
    "    \n",
    "            if word_corrected == None:\n",
    "                word_list_corrected.append(word)\n",
    "    \n",
    "            else:\n",
    "                word_list_corrected.append(word_corrected)\n",
    "        else:\n",
    "            word_list_corrected.append(word)\n",
    "    \n",
    "    text_corrected = \" \".join(word_list_corrected)\n",
    "    \n",
    "    return text_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text_stemmer = lambda text: \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying Lemmatization\n",
    "\n",
    "import spacy\n",
    "\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "\n",
    "text_lemmatizer = lambda text : \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing non alphabetic words\n",
    "\n",
    "discard_non_alpha = lambda text : \" \".join([word for word in regexp.tokenize(text) if word.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retainment of revelant POS\n",
    "import nltk\n",
    "\n",
    "def keep_pos(text):\n",
    "    tokens = regexp.tokenize(text)\n",
    "    \n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW', 'PRP', 'PRPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WPS', 'WRB']\n",
    "    \n",
    "    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n",
    "    \n",
    "    return \" \".join(keep_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing additional stop words\n",
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\n",
    "\n",
    "others = [\"ã\", \"å\", \"ì\", \"û\", \"ûªm\", \"ûó\", \"ûò\", \"ìñ\", \"ûªre\", \"ûªve\", \"ûª\", \"ûªs\", \"ûówe\"]\n",
    "\n",
    "additional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others\n",
    "\n",
    "remove_additional_stopwords = lambda text : \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    \n",
    "    text = remove_whitespace(text)\n",
    "    \n",
    "    text = re.sub('\\n' , '', text) # converting text to one line\n",
    "    \n",
    "    text = re.sub('\\[.*?\\]', '', text) # removing square brackets\n",
    "    \n",
    "    text = remove_http(text)\n",
    "    \n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    text = remove_html(text)\n",
    "    \n",
    "    text = remove_emoji(text)\n",
    "    \n",
    "    text = convert_acronyms(text)\n",
    "    \n",
    "    text = convert_contractions(text)\n",
    "    \n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    # text = pyspellchecker(text)\n",
    "    \n",
    "    text = text_lemmatizer(text) \n",
    "    \n",
    "    text = discard_non_alpha(text)\n",
    "    \n",
    "    text = keep_pos(text)\n",
    "    \n",
    "    text = remove_additional_stopwords(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We'll combine all functions into 1 SINGLE FUNCTION 🙂 & apply on @product #descriptions https://en.wikipedia.org/wiki/Text_normalization\"\n",
    "print(\"Input: {}\".format(text))\n",
    "\n",
    "print(\"Output: {}\".format(text_normalizer(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the text_normalizer function on the product description\n",
    "\n",
    "data_train_norm, data_val_norm, data_test_norm = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "data_train_norm['normalized description'] = data_train['description'].apply(text_normalizer)\n",
    "data_val_norm['normalized description'] = data_val['description'].apply(text_normalizer)\n",
    "data_test_norm['normalized description'] = data_test['description'].apply(text_normalizer)\n",
    "\n",
    "data_train_norm['label'] = data_train['label']\n",
    "data_val_norm['label'] = data_val['label']\n",
    "data_test_norm['label'] = data_test['label']\n",
    "\n",
    "data_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and labels\n",
    "X_train_norm, y_train = data_train_norm['normalized description'].tolist(), data_train_norm['label'].tolist()\n",
    "\n",
    "X_val_norm, y_val = data_val_norm['normalized description'].tolist(), data_val_norm['label'].tolist()\n",
    "\n",
    "X_test_norm, y_test = data_test_norm['normalized description'].tolist(), data_test_norm['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfidfVec = TfidfVectorizer(ngram_range = (1, 1))\n",
    "\n",
    "X_train_tfidf = TfidfVec.fit_transform(X_train_norm)\n",
    "\n",
    "X_val_tfidf = TfidfVec.transform(X_val_norm)\n",
    "\n",
    "X_test_tfidf = TfidfVec.transform(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train models with TFIDF vectorization \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"KNN Classifier\",\n",
    "    \"Decision Tree\",\n",
    "    \"Linear SVM\",\n",
    "    \"Random Forest\",\n",
    "    \"SGD Classifier\",\n",
    "    \"Ridge Classifier\",\n",
    "    \"XGBoost\",\n",
    "    \"AdaBoost\",\n",
    "]\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(max_iter = 1000),\n",
    "    KNeighborsClassifier(n_neighbors = 149, n_jobs = -1),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(kernel = 'linear'),\n",
    "    RandomForestClassifier(n_estimators = 100),\n",
    "    SGDClassifier(loss = 'hinge'),\n",
    "    RidgeClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier()\n",
    "]\n",
    "\n",
    "def score(X_train, y_train, X_val, y_val, names = names, models = models):\n",
    "    score_df, score_train, score_val = pd.DataFrame(), [], []\n",
    "    \n",
    "    x = time()\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        y_train_pred, y_val_pred = model.predict(X_train), model.predict(X_val)\n",
    "    \n",
    "        score_train.append(accuracy_score(y_train, y_train_pred))\n",
    "    \n",
    "        score_val.append(accuracy_score(y_val, y_val_pred))\n",
    "    \n",
    "    score_df[\"Classifier\"], score_df[\"Training accuracy\"], score_df[\"Validation accuracy\"] = names, score_train, score_val\n",
    "    \n",
    "    score_df.sort_values(by = 'Validation accuracy', ascending = False, inplace = True)\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of baseline models\n",
    "score(X_train_tfidf, y_train, X_val_tfidf, y_val, names = names, models = models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameter tuning with TFIDF\n",
    "\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# svm_classifier = SVC()\n",
    "\n",
    "# params_svm = {\n",
    "#     'kernel': ['linear'],\n",
    "#     'C': [0.1, 1, 10, 100]\n",
    "# }\n",
    "\n",
    "# best_model_svm, best_params_svm, best_score_svm, count = svm_classifier, ParameterGrid(params_svm)[0], 0, 0\n",
    "\n",
    "# for g in ParameterGrid(params_svm):\n",
    "#     time_start = time()\n",
    "    \n",
    "#     count += 1\n",
    "    \n",
    "#     print(f\"Gridpoint #{count}: {g}\")\n",
    "    \n",
    "#     svm_classifier.set_params(**g)\n",
    "    \n",
    "#     svm_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "#     y_train_pred, y_val_pred = svm_classifier.predict(X_train_tfidf), svm_classifier.predict(X_val_tfidf)\n",
    "    \n",
    "#     score_train, score_val = accuracy_score(y_train, y_train_pred), accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "#     time_stop = time()\n",
    "    \n",
    "#     m, s = int(time_stop - time_start) // 60, int(time_stop - time_start) % 60\n",
    "    \n",
    "#     print(f\"Training accuracy: {score_train}, Validation accuracy: {score_val}, Runtime: {m}m{s}s\")\n",
    "    \n",
    "#     print(\" \")\n",
    "    \n",
    "#     if score_val > best_score_svm:\n",
    "#         best_params_svm, best_score_svm = g, score_val\n",
    "\n",
    "# best_model_tfidf, best_params_tfidf, best_score_tfidf = SVC(), best_params_svm, best_score_svm\n",
    "\n",
    "# best_model_tfidf.set_params(**best_params_tfidf)\n",
    "\n",
    "# print(f\"Best model: {best_model_tfidf}\")\n",
    "\n",
    "# print(\" \") \n",
    "\n",
    "# print(f\"Best parameters: {best_params_tfidf}\")\n",
    "\n",
    "# print(f\"Best validation accuracy: {best_score_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_params = {\n",
    "    'kernel': ['linear'],\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "svm_grid = GridSearchCV(estimator=svm_classifier,param_grid=svm_params,verbose=3,cv=5,n_jobs=-1,scoring=\"accuracy\")\n",
    "\n",
    "svm_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"SVM best params : {svm_grid.best_params_}\")\n",
    "\n",
    "print(f\"SVM best score : {svm_grid.best_score_}\")\n",
    "\n",
    "svm_classifier.set_params(**svm_grid.best_params_)\n",
    "\n",
    "best_model_tfidf, best_params_tfidf, best_score_tfidf = SVC(), svm_grid.best_params_, svm_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2Vec Model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"english_contractions.json\",typ=\"series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Preprocessing for Word2Vec Model\n",
    "\n",
    "convert_to_lowercase_1 = lambda text : text.lower()\n",
    "\n",
    "contractions_dict = pd.read_json(\"english_contractions.json\", typ = 'series')\n",
    "\n",
    "contractions_list = list(contractions_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    \n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "    \n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization for Word2Vec\n",
    "for df in [data_train, data_val, data_test]:\n",
    "    df['tokens'] = (df[\"description\"].apply(convert_to_lowercase)\n",
    "                                     .apply(convert_contractions)\n",
    "                                     .apply(regexp.tokenize))\n",
    "data_train[['tokens', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Embedding\n",
    "import gensim\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches as mpatches\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing = False, k = 300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    \n",
    "    length = len(vectorized)\n",
    "    \n",
    "    summed = np.sum(vectorized, axis = 0)\n",
    "    \n",
    "    averaged = np.divide(summed, length)\n",
    "    \n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, tokens, generate_missing = False):\n",
    "    embeddings = tokens.apply(lambda x: get_average_word2vec(x, vectors, generate_missing = generate_missing))\n",
    "    \n",
    "    return list(embeddings)\n",
    "\n",
    "def plot_embedding(X, y):\n",
    "    truncated_SVD = TruncatedSVD(n_components = 2)\n",
    "    \n",
    "    truncated_SVD.fit(X)\n",
    "    \n",
    "    scores = truncated_SVD.transform(X)\n",
    "    \n",
    "    color_mapper = {label:idx for idx, label in enumerate(set(y))}\n",
    "    \n",
    "    color_column = [color_mapper[label] for label in y]\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'black']\n",
    "        \n",
    "    plt.scatter(scores[:, 0], scores[:, 1], s = 8, alpha = 0.8, c = y, cmap = ListedColormap(colors))\n",
    "    \n",
    "    red_patch = mpatches.Patch(color = 'red', label = 'Electronics')\n",
    "    \n",
    "    blue_patch = mpatches.Patch(color = 'blue', label = 'Household')\n",
    "    \n",
    "    green_patch = mpatches.Patch(color = 'green', label = 'Books')\n",
    "    \n",
    "    black_patch = mpatches.Patch(color = 'black', label = 'Clothing & Accessories')\n",
    "    \n",
    "    plt.legend(handles = [red_patch, blue_patch, green_patch, black_patch], prop = {\"size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec embedding\n",
    "X_train_embed = get_word2vec_embeddings(word2vec, data_train['tokens'])\n",
    "X_val_embed = get_word2vec_embeddings(word2vec, data_val['tokens'])\n",
    "X_test_embed = get_word2vec_embeddings(word2vec, data_test['tokens'])\n",
    "\n",
    "fig = plt.figure(figsize = (8, 7))          \n",
    "plot_embedding(X_train_embed, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to Compressed Sparse Row matrix\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "X_train_w2v = csr_matrix(X_train_embed)\n",
    "\n",
    "X_val_w2v = csr_matrix(X_val_embed)\n",
    "\n",
    "X_test_w2v = csr_matrix(X_test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training models with Word2Vec\n",
    "\n",
    "score(X_train_w2v, y_train, X_val_w2v, y_val, names = names, models = models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Word2Vec hyperparameter tuning\n",
    "\n",
    "# xgb = XGBClassifier()\n",
    "\n",
    "# params_xgb = {\n",
    "#     'learning_rate': [0.03, 0.3],\n",
    "#     'min_child_weight': [0, 10],\n",
    "#     'n_estimators': [200],\n",
    "#     'reg_lambda': [1, 2],\n",
    "#     'seed': [40]\n",
    "# }\n",
    "\n",
    "# best_model_xgb, best_params_xgb, best_score_xgb, count = xgb, ParameterGrid(params_xgb)[0], 0, 0\n",
    "\n",
    "# for g in ParameterGrid(params_xgb):\n",
    "#     time_start = time()\n",
    "\n",
    "#     count += 1\n",
    "\n",
    "#     print(f\"Gridpoint #{count}: {g}\")\n",
    "\n",
    "#     xgb.set_params(**g)\n",
    "\n",
    "#     xgb.fit(X_train_w2v, y_train)\n",
    "\n",
    "#     y_train_pred, y_val_pred = xgb.predict(X_train_w2v), xgb.predict(X_val_w2v)\n",
    "\n",
    "#     score_train, score_val = accuracy_score(y_train, y_train_pred), accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "#     time_stop = time()\n",
    "\n",
    "#     m, s = int(time_stop - time_start) // 60, int(time_stop - time_start) % 60\n",
    "\n",
    "#     print(f\"Training accuracy: {score_train}, Validation accuracy: {score_val}, Runtime: {m}m{s}s\")\n",
    "\n",
    "#     print(\" \")\n",
    "\n",
    "#     if score_val > best_score_xgb:\n",
    "#         best_params_xgb, best_score_xgb = g, score_val\n",
    "\n",
    "\n",
    "# best_model_w2v, best_params_w2v, best_score_w2v = XGBClassifier(), best_params_xgb, best_score_xgb\n",
    "\n",
    "# best_model_w2v.set_params(**best_params_w2v)\n",
    "\n",
    "# print(f\"Best model: {best_model_w2v}\")\n",
    "\n",
    "# print(\" \")\n",
    "\n",
    "# print(f\"Best parameters: {best_params_w2v}\")\n",
    "\n",
    "# print(f\"Best validation accuracy: {best_score_w2v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.03, 0.3],\n",
    "    'min_child_weight': [0, 10],\n",
    "    'n_estimators': [200],\n",
    "    'reg_lambda': [1, 2],\n",
    "    'seed': [40]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(estimator=xgb_classifier,param_grid=xgb_params,verbose=3,cv=5,n_jobs=-1,scoring=\"accuracy\")\n",
    "\n",
    "xgb_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"SVM best params : {xgb_grid.best_params_}\")\n",
    "\n",
    "xgb_classifier.set_params(**xgb_grid.best_params_)\n",
    "\n",
    "best_model_w2v, best_params_w2v, best_score_w2v = XGBClassifier(), xgb_grid.best_params_, xgb_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute and print confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def conf_mat(y_test, y_test_pred, figsize = (10, 8), font_scale = 1.2, annot_kws_size = 16):\n",
    "    class_names = [0, 1, 2, 3] # ['Electronics', 'Household', 'Books', 'Clothing & Accessories']\n",
    "    \n",
    "    tick_marks_y = [0.5, 1.5, 2.5, 3.5]\n",
    "    \n",
    "    tick_marks_x = [0.5, 1.5, 2.5, 3.5]\n",
    "    \n",
    "    confusion_matrix_ = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    confusion_matrix_df = pd.DataFrame(confusion_matrix_, range(4), range(4))\n",
    "    \n",
    "    plt.figure(figsize = figsize)\n",
    "    \n",
    "    sns.set(font_scale = font_scale) # label size\n",
    "    \n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    \n",
    "    sns.heatmap(confusion_matrix_df, annot = True, annot_kws = {\"size\": annot_kws_size}, fmt = 'd') # font size\n",
    "    \n",
    "    plt.yticks(tick_marks_y, class_names, rotation = 'vertical')\n",
    "    \n",
    "    plt.xticks(tick_marks_x, class_names, rotation = 'horizontal')\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    \n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.grid(False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_score_tfidf >= best_score_w2v:\n",
    "    best_model, X_train_vec, X_test_vec = best_model_tfidf, X_train_tfidf, X_test_tfidf\n",
    "    \n",
    "else:\n",
    "    best_model, X_train_vec, X_test_vec = best_model_w2v, X_train_w2v, X_test_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and evaluation on test set\n",
    "best_model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "score_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(pd.Series({\"Test accuracy\": score_test}).to_string())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "conf_mat(y_test, y_test_pred, figsize = (10, 8), font_scale = 1.2, annot_kws_size = 16) # Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "786e61aa4275cc3192f366f55b7b0b680e79526cf5813729beec02ee15d8da4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
